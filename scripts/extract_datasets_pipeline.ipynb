{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract BioProject datasets pipeline\n",
    "\n",
    "This notebook provides reusable helpers for fetching BioProject metadata, mapping BioSamples and SRA runs, and persisting the resulting tables. It is intended to be imported as a module or run interactively inside Jupyter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "1. Configure the Entrez email address with `setup_entrez` (or by calling `run_pipeline`).\n",
    "2. Query BioProject using `search_bioprojects` and fetch project records with `fetch_bioproject_details`.\n",
    "3. Persist BioProject, BioSample, and SRA metadata using the provided helper functions.\n",
    "4. Call `run_pipeline` for an end-to-end execution that writes TSV outputs to a directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Utilities for extracting BioProject datasets from NCBI.\"\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import Iterable, Sequence\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "from Bio import Entrez\n",
    "\n",
    "\n",
    "BIOPROJECT_QUERY_DEFAULT = '(\"Exaiptasia diaphana\"[Organism] AND microbiome)'\n",
    "DEFAULT_CHUNK_SIZE = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_entrez(email: str) -> None:\n",
    "    \"\"\"Configure the Entrez client with a contact email address.\"\"\"\n",
    "\n",
    "    Entrez.email = email\n",
    "\n",
    "\n",
    "def search_bioprojects(query: str, retmax: int = 100) -> Dict:\n",
    "    \"\"\"Run an Entrez search against the BioProject database.\"\"\"\n",
    "\n",
    "    handle = Entrez.esearch(db=\"bioproject\", term=query, retmax=retmax)\n",
    "    try:\n",
    "        return Entrez.read(handle)\n",
    "    finally:\n",
    "        handle.close()\n",
    "\n",
    "\n",
    "def fetch_bioproject_details(id_list: Sequence[str]) -> List[Dict]:\n",
    "    \"\"\"Fetch metadata for a sequence of BioProject identifiers.\"\"\"\n",
    "\n",
    "    if not id_list:\n",
    "        return []\n",
    "\n",
    "    handle = Entrez.efetch(\n",
    "        db=\"bioproject\",\n",
    "        id=\",\".join(id_list),\n",
    "        rettype=\"docsum\",\n",
    "        retmode=\"xml\",\n",
    "    )\n",
    "    try:\n",
    "        document = Entrez.read(handle)[\"DocumentSummarySet\"][\"DocumentSummary\"]\n",
    "    finally:\n",
    "        handle.close()\n",
    "    return document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_project_data_types(project: Dict) -> List[str]:\n",
    "    \"\"\"Derive the sequencing or assay data types associated with a BioProject.\"\"\"\n",
    "\n",
    "    data_types: List[str] = []\n",
    "    data_type_set = project.get(\"ProjectDataTypeSet\")\n",
    "    if isinstance(data_type_set, dict):\n",
    "        project_data = data_type_set.get(\"ProjectDataType\")\n",
    "        if isinstance(project_data, dict):\n",
    "            project_data = [project_data]\n",
    "        if isinstance(project_data, list):\n",
    "            for item in project_data:\n",
    "                if not isinstance(item, dict):\n",
    "                    continue\n",
    "                candidates = []\n",
    "                for key in (\"DataType\", \"data_type\", \"Data_Type\"):\n",
    "                    value = item.get(key)\n",
    "                    if value:\n",
    "                        candidates.append(value)\n",
    "                if not candidates:\n",
    "                    value = item.get(\"#text\") or item.get(\"@value\")\n",
    "                    if value:\n",
    "                        candidates.append(value)\n",
    "                for value in candidates:\n",
    "                    if isinstance(value, dict):\n",
    "                        value = value.get(\"#text\") or value.get(\"@value\")\n",
    "                    if isinstance(value, str):\n",
    "                        normalized = value.strip()\n",
    "                        if normalized and normalized not in data_types:\n",
    "                            data_types.append(normalized)\n",
    "    if not data_types:\n",
    "        project_type = project.get(\"ProjectType\")\n",
    "        if isinstance(project_type, dict):\n",
    "            for key in (\"ProjectTypeSubmission\", \"SubmissionType\", \"#text\", \"@value\"):\n",
    "                value = project_type.get(key)\n",
    "                if isinstance(value, dict):\n",
    "                    value = value.get(\"#text\") or value.get(\"@value\")\n",
    "                if isinstance(value, str):\n",
    "                    normalized = value.strip()\n",
    "                    if normalized:\n",
    "                        data_types.append(normalized)\n",
    "                        break\n",
    "    return data_types\n",
    "\n",
    "\n",
    "def write_bioproject_table(projects: Iterable[Dict], path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Write BioProject metadata to a TSV file and return it as a DataFrame.\"\"\"\n",
    "\n",
    "    columns = [\n",
    "        \"ProjectRecordId\",\n",
    "        \"ProjectAccession\",\n",
    "        \"ProjectDate\",\n",
    "        \"ProjectTitle\",\n",
    "        \"ProjectDescription\",\n",
    "        \"OrganismName\",\n",
    "        \"OrganismStrain\",\n",
    "        \"ProjectDataTypes\",\n",
    "    ]\n",
    "    records = []\n",
    "    for project in projects:\n",
    "        data_types = extract_project_data_types(project)\n",
    "        records.append(\n",
    "            {\n",
    "                \"ProjectRecordId\": project.get(\"Project_Id\", \"\"),\n",
    "                \"ProjectAccession\": project.get(\"Project_Acc\", \"\"),\n",
    "                \"ProjectDate\": project.get(\"Registration_Date\", \"\"),\n",
    "                \"ProjectTitle\": project.get(\"Project_Title\", \"\"),\n",
    "                \"ProjectDescription\": project.get(\"Project_Description\", \"\"),\n",
    "                \"OrganismName\": project.get(\"Organism_Name\", \"\"),\n",
    "                \"OrganismStrain\": project.get(\"Organism_Strain\", \"\"),\n",
    "                \"ProjectDataTypes\": \"; \".join(data_types) if data_types else \"\",\n",
    "            }\n",
    "        )\n",
    "    df = pd.DataFrame(records, columns=columns)\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(path, sep=\"\t\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_bioproject_table(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load a BioProject table that was previously written to disk.\"\"\"\n",
    "\n",
    "    return pd.read_csv(path, sep=\"\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _chunked(sequence: Sequence[str], size: int = DEFAULT_CHUNK_SIZE) -> Iterable[Sequence[str]]:\n",
    "    for start in range(0, len(sequence), size):\n",
    "        yield sequence[start : start + size]\n",
    "\n",
    "\n",
    "def map_bioproject_to_biosample(project_ids: Sequence[str]) -> pd.DataFrame:\n",
    "    \"\"\"Link BioProjects to BioSamples via Entrez elink.\"\"\"\n",
    "\n",
    "    records: list[dict[str, str]] = []\n",
    "    for chunk in _chunked(list(project_ids)):\n",
    "        handle = Entrez.elink(dbfrom=\"bioproject\", db=\"biosample\", id=chunk)\n",
    "        try:\n",
    "            links = Entrez.read(handle)\n",
    "        finally:\n",
    "            handle.close()\n",
    "        for entry in links:\n",
    "            source_id = entry.get(\"IdList\", [\"\"])[0]\n",
    "            for linkset in entry.get(\"LinkSetDb\", []) or []:\n",
    "                if linkset.get(\"DbTo\") != \"biosample\":\n",
    "                    continue\n",
    "                for link in linkset.get(\"Link\", []) or []:\n",
    "                    target = link.get(\"Id\")\n",
    "                    if target:\n",
    "                        records.append(\n",
    "                            {\n",
    "                                \"BioProjectRecordId\": str(source_id),\n",
    "                                \"BioSampleRecordId\": str(target),\n",
    "                            }\n",
    "                        )\n",
    "    df = pd.DataFrame(records, columns=[\"BioProjectRecordId\", \"BioSampleRecordId\"])\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def map_bioproject_to_srr(project_ids: Sequence[str]) -> pd.DataFrame:\n",
    "    \"\"\"Link BioProjects to SRA run accessions via Entrez elink.\"\"\"\n",
    "\n",
    "    records: list[dict[str, str]] = []\n",
    "    for chunk in _chunked(list(project_ids)):\n",
    "        handle = Entrez.elink(dbfrom=\"bioproject\", db=\"sra\", id=chunk)\n",
    "        try:\n",
    "            links = Entrez.read(handle)\n",
    "        finally:\n",
    "            handle.close()\n",
    "        for entry in links:\n",
    "            source_id = entry.get(\"IdList\", [\"\"])[0]\n",
    "            for linkset in entry.get(\"LinkSetDb\", []) or []:\n",
    "                if linkset.get(\"DbTo\") != \"sra\":\n",
    "                    continue\n",
    "                for link in linkset.get(\"Link\", []) or []:\n",
    "                    target = link.get(\"Id\")\n",
    "                    if target:\n",
    "                        records.append(\n",
    "                            {\n",
    "                                \"BioProjectRecordId\": str(source_id),\n",
    "                                \"SraRunId\": str(target),\n",
    "                            }\n",
    "                        )\n",
    "    df = pd.DataFrame(records, columns=[\"BioProjectRecordId\", \"SraRunId\"])\n",
    "    if not df.empty:\n",
    "        df = df.drop_duplicates().reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_srr_details(id_list: Sequence[str]) -> pd.DataFrame:\n",
    "    \"\"\"Fetch run metadata for a collection of SRA accessions.\"\"\"\n",
    "\n",
    "    if not id_list:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    frames: list[pd.DataFrame] = []\n",
    "    for chunk in _chunked(list(id_list)):\n",
    "        handle = Entrez.efetch(db=\"sra\", id=\",\".join(chunk), rettype=\"runinfo\", retmode=\"text\")\n",
    "        try:\n",
    "            frame = pd.read_csv(handle)\n",
    "        finally:\n",
    "            handle.close()\n",
    "        if not frame.empty:\n",
    "            frames.append(frame)\n",
    "    if not frames:\n",
    "        return pd.DataFrame()\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "def write_bioproject_srr_mapping(mapping: pd.DataFrame, path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Persist the BioProject-to-SRA mapping as a TSV file.\"\"\"\n",
    "\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    mapping.to_csv(path, sep=\"\t\", index=False)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def summarize_samples(srr_details: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Summarize SRA runs per BioProject.\"\"\"\n",
    "\n",
    "    if srr_details.empty:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"BioProject\", \"RunCount\", \"BioSampleCount\", \"LibraryStrategies\", \"LibraryLayouts\"]\n",
    "        )\n",
    "\n",
    "    def _collect_unique(series: pd.Series) -> str:\n",
    "        unique_values = sorted({value for value in series.dropna() if str(value).strip()})\n",
    "        return \"; \".join(unique_values)\n",
    "\n",
    "    summary = (\n",
    "        srr_details.groupby(\"BioProject\", dropna=False)\n",
    "        .agg(\n",
    "            RunCount=(\"Run\", \"nunique\"),\n",
    "            BioSampleCount=(\"BioSample\", \"nunique\"),\n",
    "            LibraryStrategies=(\"LibraryStrategy\", _collect_unique),\n",
    "            LibraryLayouts=(\"LibraryLayout\", _collect_unique),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_sra_accession(value: str) -> str:\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    value_str = str(value)\n",
    "    if value_str.startswith(\"SRR\"):\n",
    "        return value_str\n",
    "    if value_str.isdigit():\n",
    "        return f\"SRR{value_str}\"\n",
    "    return value_str\n",
    "\n",
    "\n",
    "def run_pipeline(\n",
    "    email: str,\n",
    "    output_dir: Path,\n",
    "    query: str = BIOPROJECT_QUERY_DEFAULT,\n",
    "    retmax: int = 100,\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Execute the full BioProject to SRA extraction pipeline.\"\"\"\n",
    "\n",
    "    setup_entrez(email)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    search_results = search_bioprojects(query=query, retmax=retmax)\n",
    "    project_ids = search_results.get(\"IdList\", [])\n",
    "\n",
    "    projects = fetch_bioproject_details(project_ids)\n",
    "    bioproject_table = write_bioproject_table(projects, output_dir / \"bioproject_table.tsv\")\n",
    "\n",
    "    record_to_accession = {\n",
    "        str(project.get(\"Project_Id\", \"\")): project.get(\"Project_Acc\", \"\") for project in projects\n",
    "    }\n",
    "\n",
    "    biosample_map = map_bioproject_to_biosample(project_ids)\n",
    "    if not biosample_map.empty:\n",
    "        biosample_map = biosample_map.assign(\n",
    "            BioProjectAccession=biosample_map[\"BioProjectRecordId\"]\n",
    "            .map(record_to_accession)\n",
    "            .fillna(\"\"),\n",
    "        )\n",
    "\n",
    "    srr_map = map_bioproject_to_srr(project_ids)\n",
    "    if not srr_map.empty:\n",
    "        srr_map = srr_map.assign(\n",
    "            BioProjectAccession=srr_map[\"BioProjectRecordId\"]\n",
    "            .map(record_to_accession)\n",
    "            .fillna(\"\"),\n",
    "            SraRunAccession=srr_map[\"SraRunId\"].map(_normalize_sra_accession),\n",
    "        )\n",
    "        write_bioproject_srr_mapping(srr_map, output_dir / \"bioproject_srr_mapping.tsv\")\n",
    "\n",
    "    srr_accessions = [] if srr_map.empty else srr_map[\"SraRunAccession\"].tolist()\n",
    "    srr_details = fetch_srr_details(srr_accessions)\n",
    "    srr_details_path = output_dir / \"bioproject_srr_details.tsv\"\n",
    "    if not srr_details.empty:\n",
    "        srr_details.to_csv(srr_details_path, sep=\"\t\", index=False)\n",
    "\n",
    "    sample_summary = summarize_samples(srr_details)\n",
    "    summary_path = output_dir / \"bioproject_sample_summary.tsv\"\n",
    "    if not sample_summary.empty:\n",
    "        sample_summary.to_csv(summary_path, sep=\"\t\", index=False)\n",
    "\n",
    "    return {\n",
    "        \"projects\": pd.DataFrame(projects),\n",
    "        \"bioproject_table\": bioproject_table,\n",
    "        \"biosample_map\": biosample_map,\n",
    "        \"srr_map\": srr_map,\n",
    "        \"srr_details\": srr_details,\n",
    "        \"sample_summary\": sample_summary,\n",
    "    }\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    \"BIOPROJECT_QUERY_DEFAULT\",\n",
    "    \"DEFAULT_CHUNK_SIZE\",\n",
    "    \"extract_project_data_types\",\n",
    "    \"fetch_bioproject_details\",\n",
    "    \"fetch_srr_details\",\n",
    "    \"load_bioproject_table\",\n",
    "    \"map_bioproject_to_biosample\",\n",
    "    \"map_bioproject_to_srr\",\n",
    "    \"run_pipeline\",\n",
    "    \"search_bioprojects\",\n",
    "    \"setup_entrez\",\n",
    "    \"summarize_samples\",\n",
    "    \"write_bioproject_srr_mapping\",\n",
    "    \"write_bioproject_table\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example execution\n",
    "\n",
    "Uncomment the cell below to run the full pipeline with your contact email and a desired output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# results = run_pipeline(\n",
    "#     email=\"lukas.becker@hhu.de\",\n",
    "#     output_dir=Path(\"../data\"),\n",
    "# )\n",
    "# results[\"sample_summary\"].head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}